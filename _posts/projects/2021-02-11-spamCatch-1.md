---
layout: post
title: Building a full-stack spam catching app - 1. Background
title-clean: Building a full-stack spam catching app <div class="a2">1. Background</div>
author: matt_sosna
image: "images/projects/spamcatch-demo.png"
---

![]({{  site.baseurl  }}/images/projects/spamcatch-demo.png)

[**SpamCatch**](https://spam-catcher.herokuapp.com) is a fun side project I did to bring together [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing), [Flask](https://flask.palletsprojects.com/en/1.1.x/), and [the front-end](https://blog.udacity.com/2014/12/front-end-vs-back-end-vs-full-stack-web-developers.html). Classifying spam text messages is a classic machine learning problem, but I'd never seen people test their classifier on raw strings of text. I'd also never seen a spam classifier hooked up to a nice user interface, where people could use the classifier without needing to know Python or Git.

There's a lot to cover, so I'll split this into two posts. This post will set the stage for what spam is and how we can build a model to automatically identify it. The [next post]({{  site.baseurl  }}/spamCatch-2) will dive deep into the actual code for the app.

**Make sure to [check out the actual app](https://spam-catcher.herokuapp.com)!** (If it takes a minute to load, that's because the dyno went to sleep. The free plan only gets you so far!) You can also view the source code [here](https://github.com/mgsosna/spamCatch).

---

<span style="font-size:18px">**1. Background** (this post)</span>
  - [Spam](#spam)
  - [Strings to vectors](#strings-to-vectors)
  - [Why random forest?](#why-random-forest)
  - [What is Flask?](#what-is-flask)<br><br>

<span style="font-size:18px">**2. How it works** ([next post]({{  site.baseurl  }}/spamCatch-2))</span>
  - [**Python**]({{  site.baseurl  }}/spamCatch-2/#python)
    - [The TF-IDF vectorizer]({{  site.baseurl  }}/spamCatch-2/#the-tf-idf-vectorizer)
    - [The classifier]({{  site.baseurl  }}/spamCatch-2/#the-spam-classifier)
    - [Flask]({{  site.baseurl  }}/spamCatch-2/#flask)
  - [**The front-end**]({{  site.baseurl  }}/spamCatch-2/#the-front-end)
    - [HTML]({{  site.baseurl  }}/spamCatch-2/#html)
    - [JavaScript]({{  site.baseurl  }}/spamCatch-2/#javascript)
  - [**Deployment**]({{  site.baseurl  }}/spamCatch-2/#deployment)


---

### Spam
Spam messages are at best a nuisance and [at worst dangerous](https://www.consumer.ftc.gov/articles/how-recognize-and-avoid-phishing-scams). While malicious spam can be carefully tailored to the recipient to sound credible, the vast majority of spam out there is easily identifiable crap. You usually don't need to even read the entire message to identify it as spam $-$ there's a sense of urgency, money needed or offered, a sketchy link to click.

If spam is so predictable, let's just write some code to automatically distinguish it from normal messages (also called "ham") and toss it in the trash. Our first guess might be to write a bunch of rules, a series of `if` statements that get triggered when our classifier sees certain words in the message. Accepting the risk of losing out on a once in a lifetime, all-expenses-paid vacation, I could configure my classifier to automatically delete any message with the word `free` in it.

<center>
<img src="{{  site.baseurl  }}/images/projects/classifier1.png" height="70%" width="70%" style="margin-top: -20px">
</center>

But that's not quite right... yes, the word `free` pops up a lot in spam, but it also appears in normal speech all the time, too. (*"Hey, are you free tonight?"*, for example.) We need more rules... lots more rules.

<center>
<img src="{{  site.baseurl  }}/images/projects/classifier2.png">
</center>

Our classifier is much more complicated and barely more accurate. In fact, it would take hundreds of hours of manually writing such a decision tree to make our classifier actually worthwhile. We'd need hundreds or thousands of `if` statements to be able to distinguish more subtle spam messages. We'd want the `if` statement logic to be informed by research on *how frequently* certain words appear in spam versus ham. Finally, we'd probably want our branches to increase or decrease a *probability of spam* rather than needing to hard-code "spam" vs. "ham" outcomes into certain branch trajectories. But most challenging of all... **we'd need to write all of this ourselves!**

<div style="text-align: center; font-weight: bold">
Quick! Click on <a src="https://en.wikipedia.org/wiki/Natural_language_processing">this link</a> to find a better way!
</div>

Just kidding. But that link *does* point us to a tempting alternative $-$ the field of NLP, or [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing). NLP is a subfield of artificial intelligence that uses computational techniques to understand human language. In essence, **NLP converts words to *numbers* so we can do math on them.** With NLP, we can reinterpret our messages as *vectors of numbers*, then train a machine learning classifier to identify patterns in the vectors that distinguish spam from normal messages.

Finally, we need some data. We *could* sort through our own spam messages and text all our friends for theirs... but that's a lot of work. (Our strange requests might also end up in their own spam!) Instead, let's use the [spam message dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset) from Kaggle, a classic dataset for NLP classification problems.

### Strings to vectors
We first need to decide what kind of vector to turn each text message into. The simplest approach would be to create a [**bag of words**](https://towardsdatascience.com/a-simple-explanation-of-the-bag-of-words-model-b88fc4f4971) from our *documents* (a more general term for our text samples). In a bag of words approach, we first identify the *vocabulary* of unique words in our set of documents, then create a vector of word frequencies for each document. If our training set consisted of the three documents below, for example, our vocabulary would be `the`, `cat`, `sits`, `is`, and `black`, and we could categorize each document by how frequently each word appears.

| **Document**                     |&nbsp;**the**&nbsp;|&nbsp;**cat**&nbsp;|&nbsp;**sits**&nbsp;|&nbsp;**is**&nbsp;| &nbsp; **black**&nbsp;   |
|----------------------------------|--------------|-------------|-------------|-------------|-------------|
| &nbsp;*the cat sits*&nbsp;       | &nbsp;1&nbsp;|&nbsp;1&nbsp;|&nbsp;1&nbsp;|&nbsp;0&nbsp;|&nbsp;0&nbsp;|
| &nbsp;*the cat is black* &nbsp;  | &nbsp;1&nbsp;|&nbsp;1&nbsp;|&nbsp;0&nbsp;|&nbsp;1&nbsp;|&nbsp;1&nbsp;|
| &nbsp;*the black cat sits*&nbsp; | &nbsp;1&nbsp;|&nbsp;1&nbsp;|&nbsp;1&nbsp;|&nbsp;0&nbsp;|&nbsp;1&nbsp;|
{:.mbtablestyle}

<span style="font-size:12px"><i>Inspired by [Victor Zhou](https://towardsdatascience.com/a-simple-explanation-of-the-bag-of-words-model-b88fc4f4971)</i></span>

But these "term frequency" vectors created by a bag of words aren't *that* informative. Yes, they tell us how many times the word `cat` appears in a document, for example. But knowing that `cat` appears once in *"the cat sits"* becomes meaningless when you realize `cat` appears once in *every* document! In fact, unless we looked at all the other documents, we wouldn't know whether `cat` appearing 100 or 1,000 times in a document is informative at all.<sup>[[1]](#1-strings-to-vectors)</sup>

It's therefore better to weight our term frequency vectors by **how frequently the terms occur across *all* documents**. If every document says the word `cat` 100 times, it's no big deal $-$ but if your document is the *only* one to mention `cat`, that's incredibly informative! These weighted vectors are called **term frequency - inverse document frequency (TF-IDF)** vectors.

Finally, we'll also want to remove **stop words** and perform **lemmatization.** Stop words are words like `the`, `and`, `if`, etc. whose main purpose is linguistic logic. Stop words don't contain information about the *content* of the document, so they just make it harder for a model to discriminate between documents.<sup>[[2]](#2-strings-to-vectors)</sup> Similarly, the words `eating`, `eats`, and `ate` look like entirely different terms to an NLP model when they're really just different ways of saying `eat`. [Lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) is the process of stripping that linguistic layer off the root of each word.

When we remove stop words, perform lemmatization, and weight the above term frequency vectors by their document frequencies, we get these TF-IDF vectors:

| **Document**                    | **black**  | **cat**   | **sit**   |
|---------------------------------|-----------|-----------|-------|
| &nbsp;*the cat sits*&nbsp;      | &nbsp;0.000&nbsp;  |&nbsp; 0.613  &nbsp;   | &nbsp;0.790&nbsp; |
| &nbsp;*the cat is black* &nbsp; | 0.790     | 0.613     | 0.000 |
| &nbsp;*the black cat sits*&nbsp;| 0.620     | 0.481     | 0.620 |
{:.mbtablestyle}

The values are now a lot less intuitive for us, but they're much more informative to an algorithm trying to discern between the documents.

### Why random forest?
The TF-IDF vectors in the table above are only three elements long, since our slimmed-down vocabulary only consists of the words `black`, `cat`, and `sit`. There are also few zeros in the vectors $-$ all vectors have at least 2/3 of all words in the vocabulary.

To actually catch spam, we'll want a vocabulary with thousands of words. TF-IDF vectors trained on this vocabulary will mostly consist of zeros, since not every document will include every word in our training set. Such high-dimensional and sparse (mostly-zero) vectors are difficult for classical statistics approaches.<sup>[[3]](#3-why-random-forest)</sup> We also care less about understanding exactly *how* our model catches spam $-$ we just want the most accurate predictor possible.

We'll therefore want to use machine learning. My first choice is usually a [random forest](https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/) algorithm unless I need something more specialized. A random forest consists of a series of decision trees fit to [bootstrapped](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) subsets of your data. Individual trees tend to become overfit to their training data, but these errors average out across all trees, resulting in an [ensemble](https://en.wikipedia.org/wiki/Ensemble_learning) that can generate surprisingly accurate predictions.<sup>[[4]](#4-why-random-forest)</sup>

![]({{  site.baseurl  }}/images/projects/random_forest.png)
<span style="font-size:12px"><i>Source: [Kaggle](https://www.kaggle.com/getting-started/176257)</i></span>

### What is Flask?
One more concept before we start building our app. It's one thing to have an amazing model tucked away in a Jupyter notebook hidden in your computer, and entirely another to have that model accessible to the world. **[Flask](https://flask.palletsprojects.com/en/1.1.x/) is a Python library that lets you make code _accessible outside your current Python environment_.** With Flask, you can create a [*server*](https://techterms.com/definition/server) with functions at [*API endpoints*](https://www.mulesoft.com/resources/api/what-is-an-api).

These endpoints are the interface between your code and the outside world. They let you access your Python code while you're in another Python script... or even *when you're not using Python, but your browser.* We'll build our app so we actually interact with our Python spam prediction model on an HTML page, using JavaScript to communicate between the user and our model. Our app will mimic the flow chart below, minus the database.<sup>[[5]](#5-what-is-flask)</sup>

![]({{  site.baseurl  }}/images/projects/api-model.png)
<span style="font-size:12px"><i>Source: [ServiceObjects](https://www.serviceobjects.com/blog/what-is-an-api/)</i></span>

## Conclusions
Ways to make it better:
* Looking for patterns in the URLs themselves. `www.google.com` is fine, but `u7x0apmw.com` isn't.
* Add features like number of letters that are capitalized, number of exclamation marks

## Footnotes
#### 1. [Strings to vectors](#strings-to-vectors)
Some would consider the word `cat` appearing 100 times in a document to be... *catastrophic.*

#### 2. [Strings to vectors](#strings-to-vectors)
While grammar like stop words and punctuation distract our model from the _content_ of a document, they do still hold valuable information a more advanced model will want to incorporate. Consider [these two sentences](https://algorithmia.com/blog/advanced-grammar-and-natural-language-processing-with-syntaxnet):

> "Most of the time, travelers worry about their luggage." <br>
  "Most of the time travelers worry about their luggage."

That comma is pretty important for knowing what kind of travelers we're talking about!

#### 3. [Why random forest?](#why-random-forest)
[This article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2865881/) from *Philosophical Transactions of the Royal Society A: Mathematical, Physical, and Engineering Sciences* goes into great detail on approaches for dealing with sparse vectors. One of the issues they mention is that when the number of features is greater than the number of samples, $X^TX$ becomes singular and cannot be used to estimate model parameters.

#### 4. [Why random forest?](#why-random-forest)
The fact that ensemble methods generate predictions more accurate than individual models reminds me a lot of [collective animal behavior](https://en.wikipedia.org/wiki/Collective_animal_behavior), which my Ph.D. was on. I'll need to write a blog post nerding out on the comparisons sometime.

#### 5. [What is Flask?](#what-is-flask)
The [Flask SQLAlchemy](https://flask-sqlalchemy.palletsprojects.com/en/2.x/) library lets you set up and integrate a database into your Flask application. We could do this, for example, if we wanted to save every query users submit to our app. But be wary of [SQL injection attacks](https://www.w3schools.com/sql/sql_injection.asp)!
