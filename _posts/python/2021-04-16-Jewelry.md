---
layout: post
title: Efficient type validation for Python functions
author: matt_sosna
---

When it comes to writing complex pipelines running in production, you don't want ambiguity in what's happening at each step of the pipeline. Function \#1 takes in two strings, queries an API, and outputs a `dict`; function \#2 takes that `dict`, converts it to a `pandas` dataframe and returns a cleaned version; etc.

But despite our best efforts to write modular, well-tested functions, bugs love to hide in the nuances of these pipelines, and they can be hard to catch even with end-to-end tests.

This post will cover a Python decorator for **input validation**, which we can use to "lock" the inputs to our functions and immediately notice when there's an unexpected mismatch.

## Use case
Consider our simple pipeline from above. Let's say there's one main function a user can trigger, called `run_pipeline`. It simply takes a start and end date, then runs `query_api` and `process_dict`, before saving a CSV.

```python
import datetime as dt

def run_pipeline(start_date: dt.datetime,
                 end_date: dt.datetime):
    """
    Query API for data between dates, then process output.
    Note: end_date exclusive.
    """
    response = query_api(start_date, end_date)
    df = process_dict(response)
    df.to_csv('cleaned_data.csv', index=False)
```

Simple enough, right? Well now let's look at `query_api` and `process_dict`. There are two points of failure here, one easier to spot than the other. Can you see them?

```python
import logging
import requests
import numpy as np
import pandas as pd
import datetime as dt

TEMPLATE = "http://ourcompany.com/data/start_date={}&end_date={}"


def query_api(start_date: dt.datetime,
              end_date: dt.datetime) -> dict:
    """
    Query data API for values between selected dates.
    Note: end_date exclusive.
    """

    start_date = dt.datetime.strftime(start_date, '%Y-%m-%d')
    end_date = dt.datetime.strftime(end_date, '%Y-%m-%d')

    try:
        data = requests.get(TEMPLATE.format(start_date, end_date))
        return data.json()
    except:
        logging.error("Could not retrieve data")


def process_dict(data: dict) -> pd.DataFrame:
    """
    Convert dict to df and replace values in 'sales' column.
    """

    df = pd.DataFrame(data)
    df_filt['sales'] = df['sales'].replace({'nan': np.nan, '': np.nan})

    return df_filt
```

You might have caught the first point of failure, in `query_api`: **what if `start_date` and `end_date` aren't `datetime` objects?**

If the inputs to `query_api` aren't datetime objects, we'll fail immediately on lines 17-18, when we try to convert those objects to strings. There's nothing stopping this from happening, as `run_pipeline` (which is the function actually exposed to the user) just passes its inputs straight into `query_api` without second thought.

We could fix this first issue by checking the inputs to `run_pipeline` and making sure they're datetime objects. But what about for our second issue? Have you found it yet?

If you 



Because Python doesn't enforce the type of arguments being passed into a function, we need to write the logic ourselves for validating the inputs are correct.



When it comes to productizing data science, there's a lot of architecture *around* your core model or analysis that needs careful consideration. Maybe you have a model retrainer workflow that queries an API every night for raw data, processes that data, and then updates a model to generate forecasts for tomorrow. Unless you have a solid data engineering team, a pipeline like this can end up having many moving components, especially if you're generating models for customers with varying levels of data availability. (e.g. we don't want to keep a model if the $R^2$ is too low.)




Maybe your workflow queries an API every night at 2am, then processes the data and uses it to retrain a model that generates a forecast for tomorrow.

The main use case we'll cover is **input validation.** When building a complex pipeline, we want to "lock" the inputs at various stages. You could write a function that handles errors, e.g. some sort of private method like `_validate_inputs`, but it's often cleaner to move that logic outside the function as a decorator. Then you know anything you're reading inside the function is what you think it is.

This is also especially important when a user is involved. Let's imagine we have a class that formats a URL for querying data. We use a `DataParams` class that is effectively a template and converts inputs to values in the URL.

Somewhat similar to [data classes](https://towardsdatascience.com/data-classes-in-python-8d1a09c1294b).

One option is to write something like this:

```python
class DataParams:
    def __init__(self, start_date, end_date):
        self.start_date = start_date
        self.end_date = end_date
```

But what if we accidentally pass in a date that is a weirdly-formatted string rather than a `dt.datetime` object? Our `DataParams` class won't really work.

An alternative is to instantiate the values to `None`, then have specific functions for setting start and end date. Inside these functions, we could do type validation.

```python
class DataParams:

    def __init__(self):
        self.start_date = None
        self.end_date = None

    def set_start_date(self, start_date):
        assert isinstance(start_date, dt.datetime), \
            f"start_date must be dt.datetime"
        self.start_date = start_date

    def set_end_date(self, end_date):
        assert isinstance(end_date, dt.datetime), \
            f"end_date must be dt.datetime"
        self.end_date = end_date
```

While this works ok, it could be nicer to just be able to instantiate `DataParams` with all parameters at once and check our types there.


But what if the data isn't in the right format?


Or you get an unexpected input? Your

I created a package tentatively called `jewelry` and am looking for contributors. Have an idea for how to improve the functions I'm about to show you? Hit me up bro.

This post assumes you're somewhat familiar with **decorators**, or Python functions that *modify* other functions. Decorators serve as wrappers around a function, allowing us control over the inputs and outputs of a function without needing to modify the function itself. (For more detail, check out *this great post XXX* by XXX.)

## Jewelry

### `ArgChecker`
This is a class that we start by loading and instantiating. We can then use its `enforce_type_hints` method.

```python
from jewelry import ArgChecker
ac = ArgChecker()
```

#### `enforce_type_hints`
* Verify that inputs to a function match the type hints.

```python
def multiply(a, b):
    return a * b

@ac.enforce_type_hints
def safe_multiply(a: int, b: int):
    return a * b

# Good behavior
multiply(1, 2)       # 2
safe_multiply(1, 2)  # 2

# Diverges
multiply('1', 2)     # '11'
safe_multiply('1', 2)  # AssertionError: a is type <class 'str'>
```




### `DataChecker`
I guess this is kind of like Great Expectations, so I'd want to see how it differs... but could do something simple.

```python
def check_thing(df, cols):
    for col in cols:
        assert df[col].max() - df[col].min() > 5
```

#### `.colcheck`

```python
@ac.colcheck(df, ['a', 'b'])
def add_cols(df: pd.DataFrame):
    return df['a'] + df['b']
```

Makes sure that cols `a` and `b` are present, raises an `AssertionError` if not.


* `jewelry` is a Python repo I've created for this. Please contribute.
