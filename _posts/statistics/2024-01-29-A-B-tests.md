---
layout: post
title: A/B Testing
author: matt_sosna
tags: statistics
---

It's a lot of pressure to run a company. You've built your app up from the ground but you're considering changing something. Is it the right call? You like the new version, but will your customers? We could pay a few of them to look at it and tell you, but what if there was some way to test them in real time? What if the change is so small they can't even really notice, or the change isn't even on the frontend at all?

You might think of it as the color of a button on the frontend.

<center>
<img src="{{  site.baseurl  }}/images/statistics/a-b/a_vs_b.png" height="80%" width="80%">
</center>

But it could also be _how_ that frontend is generated: a different retrieval system for user data, for example. An A/B test here is particularly important, since users' opinions might not be reliable. (And users might not even be directly affected. Important metric but not the only one - could use proxies.)



How can we tell if a change is good? Are things the _same_ or _different_?


### Demo

{% include header-python.html %}
```python
import numpy as np

def permutation_test(
    pop_a: np.ndarray,
    pop_b: np.ndarray,
    n_iter: int = 1000
) -> float:
    """
    Returns the p-value of the difference in means
    between pop_a and pop_b.
    """
    true_diff = np.mean(pop_a) - np.mean(pop_b)

    combined = np.concatenate([pop_a, pop_b])

    diffs = []
    for _ in range(n_iter):
        np.random.shuffle(combined)
        sim_pop_a, sim_pop_b = np.split(combined, 2)
        sim_diff = np.mean(sim_pop_a) - np.mean(sim_pop_b)
        diffs.append(sim_diff)

    return min(
        np.mean(true_diff <= diffs),
        np.mean(true_diff >= diffs)
    )
```

{% include header-python.html %}
```python
pop_a = np.random.normal(0, 1, 1000)
pop_b = np.random.normal(0.1, 1, 1000)

permutation_test(pop_a, pop_b, 10000)
# 0.0019
```
## A checklist for A/B testing

### 1. What is the target metric?
What are we actually trying to change? How will we quantify the effectiveness of treatment vs. control?


Some concepts:
* **Online** vs. **offline** testing.

What's the actual test:
* [Permutation tests](https://en.wikipedia.org/wiki/Permutation_test) are non-parametric: they don't assume the data are normally distributed. Generally safer than something like a t-test.
  * Null hypothesis is that all samples come from same distribution.
  * Measure difference in means from popA and popB, then lump all the data together and randomly allocate samples to "A" and "B" and measure difference in means. Then see where your true difference falls on the distribution of differences.
* Stratified sampling: if there are factors (e.g., demographics) that could affect the outcome, should sample in a way that these groups are equally represented across treatments. (But otherwise should have random assignment.)
* Sample size must be large enough to have statistical power. Also make sure to run it long enough to handle weekly variation, etc.
* Adjust for multiple comparisons (e.g., ANOVA, Bonferroni correction)
* Think effect size: does small but significant really matter?


**A/B test Definition**
* Way to quantify differences in some metric between two versions and say whether the differences are statistically significant.
* Split traffic in two. Doesn't need to be 50-50.
  * But what is the minimum number of people we need to run an A/B test on to be able to detect differences?

**Notes**
* Businesses think they know their customers, but they can often have unexpected behavior. Customers themselves might not even know that they're subconsciously behaving a certain way.
* A/B tests are a way to guide decision-making for product launches. Evaluate some success metric on a subset of users.

**Designing the experiment**
* Simplest version: control A, treatment B.
* Should declare the metric before you begin!
  * Conversion rate, newsletter opens, ad clicks
* How long to run the test?
  * Need to determine the sample size.
    * Type II error (power), significance level, minimum detectable effect

$$N = \frac{16\sigma^2}{\delta^2}$$



**Running the experiment**
* Should just change one thing at a time when comparing so you can isolate the cause of the difference.
* As a sanity check, you can also set up an A/A test where there are actually no differences between groups. This lets you test the testing infra itself, e.g., how users are randomly allocated between groups.

Examples:
* Frontend: layout of a page (e.g., image on left/right, button is blue vs. red)
* Backend: ML algorithm serving ads, how stories are ranked in Feed, etc.


Questions:
* Is it always a comparison of discrete groups? It'd be something else if I had a continuous variable I guess.
