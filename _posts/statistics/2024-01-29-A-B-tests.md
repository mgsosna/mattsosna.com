---
layout: post
title: Why A/B Testing is Critical for ML in Prod
author: matt_sosna
tags: statistics
---

Imagine you're running an e-commerce startup and just launched a "Products You Might Like" service. The recommender system producing the suggested products seems fine, but customer engagement is way lower than you expected. The stakes feel high -- your company doesn't have much [runway](https://www.brex.com/journal/startup-runway), so getting this service right is important. You're eager to improve the product but are worried about making a change that lowers engagement even further. What do you do?

<center>
<img src="{{  site.baseurl  }}/images/statistics/a-b/intro_rec.png" height="60%" width="60%">
</center>

In this post, we'll talk about **A/B testing**, the gold standard for determining whether you should launch a change in production. We'll cover the theory, go through some code examples, and then cover caveats and pitfalls to avoid.

## Background
### How are decisions made?
Before we cover A/B testing, it's worth thinking more broadly about how people decide to launch products in general. A/B testing is at the end of a long ladder of sophistication, but that doesn't mean it's by default the best option.

#### Level 1: Just make the change
Self-explanatory. Just do it. I'll include asking around your company and getting alignment internally before launching, since unless you're a 1-person startup you're going to need to convince someone else what you're doing.

One option is to just make the change outright. If you're confident it's an improvement, then just do it. But that's risky: what if the change makes it worse? What if something breaks? What if it leads to a huge drop in user engagement? You like the new version, but will your customers?

#### Level 2: Ask a few customers
Another option is to pay some users to look at the new version and tell you. This [focus group](https://www.shopify.com/blog/what-is-a-focus-group) is useful, since you can get detailed info on the proposed changes. But it's a lot of work -- you need to carefully select participants to make sure they're representative of your app's audience, curate their feedback carefully to avoid biasing them, etc. Cost might not be a big deal but it still costs something to hire these participants. And it takes time to get your group together, get their feedback and process it. And you might end up with a lot of qualitative feedback rather than quantitative metric changes.

#### Level 3: Split traffic to each version
So what if there was some way to test changes in real time? What if the change is so small that a focus group doesn't even make sense? Or it's something a user might not really notice, like a backend change: a latency reduction of a few milliseconds when rendering an image, for example?

### Designing the experiment
### 1. What is the target metric?
What are we actually trying to change? How will we quantify the effectiveness of treatment vs. control? There are actually lots of metrics you'll want to test for: the success ones you care about (like user engagement) but a lot of ones you don't want to regress, like long-term retention, how long it takes Feed to load, user appeals, etc.


Some concepts:
* **Online** vs. **offline** testing.

What's the actual test:
* [Permutation tests](https://en.wikipedia.org/wiki/Permutation_test) are non-parametric: they don't assume the data are normally distributed. Generally safer than something like a t-test.
  * Null hypothesis is that all samples come from same distribution.
  * Measure difference in means from popA and popB, then lump all the data together and randomly allocate samples to "A" and "B" and measure difference in means. Then see where your true difference falls on the distribution of differences.
* Stratified sampling: if there are factors (e.g., demographics) that could affect the outcome, should sample in a way that these groups are equally represented across treatments. (But otherwise should have random assignment.)
* Sample size must be large enough to have statistical power. Also make sure to run it long enough to handle weekly variation, etc.
* Adjust for multiple comparisons (e.g., ANOVA, Bonferroni correction)
* Think effect size: does small but significant really matter?


**A/B test Definition**
* Way to quantify differences in some metric between two versions and say whether the differences are statistically significant.
* Split traffic in two. Doesn't need to be 50-50.
  * But what is the minimum number of people we need to run an A/B test on to be able to detect differences?

**Notes**
* Businesses think they know their customers, but they can often have unexpected behavior. Customers themselves might not even know that they're subconsciously behaving a certain way.
* A/B tests are a way to guide decision-making for product launches. Evaluate some success metric on a subset of users.

**Designing the experiment**
* Simplest version: control A, treatment B.
* Should declare the metric before you begin!
  * Conversion rate, newsletter opens, ad clicks
* How long to run the test?
  * Need to determine the sample size.
    * Type II error (power), significance level, minimum detectable effect

$$N = \frac{16\sigma^2}{\delta^2}$$

### Running the experiment
* Should just change one thing at a time when comparing so you can isolate the cause of the difference.
* As a sanity check, you can also set up an A/A test where there are actually no differences between groups. This lets you test the testing infra itself, e.g., how users are randomly allocated between groups.

Examples:
* Frontend: layout of a page (e.g., image on left/right, button is blue vs. red)
* Backend: ML algorithm serving ads, how stories are ranked in Feed, etc.


You might think of it as the color of a button on the frontend.

<center>
<img src="{{  site.baseurl  }}/images/statistics/a-b/a_vs_b.png" height="80%" width="80%">
</center>

But it could also be _how_ that frontend is generated: a different retrieval system for user data, for example. An A/B test here is particularly important, since users' opinions might not be reliable. (And users might not even be directly affected. Important metric but not the only one - could use proxies.)



How can we tell if a change is good? Are things the _same_ or _different_?
* And by how much? Is it worth actually changing things?
* Confidence intervals

### Demo

Technically, a true permutation test requires generating the distribution of _every permutation_ of samples allocated to subset A vs. subset B. But unless our datasets are tiny, this is prohibitively expensive: the time cost [grows as a function of $O(n!)$]({{  site.baseurl  }}/CSDS-Intro#big-o-notation). Instead, we can approximate the distribution by permuting the data some `n_iter` of times, where `n_iter` is decently large.

{% include header-python.html %}
```python
import numpy as np

def permutation_test(
    pop_a: np.ndarray,
    pop_b: np.ndarray,
    n_iter: int = 1000
) -> float:
    """
    Returns the p-value of the difference in means
    between pop_a and pop_b.
    """
    true_diff = np.mean(pop_a) - np.mean(pop_b)
    combined = np.concatenate([pop_a, pop_b])
    diffs = _generate_diff_dist(combined, n_iter)

    return min(
        np.mean(true_diff <= diffs),
        np.mean(true_diff >= diffs)
    )

def _generate_diff_dist(
    combined: np.ndarray,
    n_iter: int
) -> list:
    """
    Generates list of differences
    """
    diffs = []

    for _ in range(n_iter):
        np.random.shuffle(combined)
        sim_pop_a, sim_pop_b = np.split(combined, 2)
        sim_diff = np.mean(sim_pop_a) - np.mean(sim_pop_b)
        diffs.append(sim_diff)

    return diffs
```

Now if we run an actual test.

{% include header-python.html %}
```python
pop_a = np.random.normal(0, 1, 1000)
pop_b = np.random.normal(0.1, 1, 1000)

permutation_test(pop_a, pop_b, 10000)
# 0.0019
```

We can also do this for distributions that aren't Gaussian. The distribution of differences actually ends up being normal, too. Though the amount of data we require to tell whether the samples come from the same population may increase quite a bit.

{% include header-python.html %}
```python
pop_a = np.random.beta(10, 1, 1000)
pop_b = np.random.beta(10.1, 1, 1000)
permutation_test(pop_a, pop_b, 10000)
# 0.453
```

We can generate a distribution of differences and see where our true difference falls on this distribution. The proportion of the distribution that is equal to or more extreme than our true difference can be interpreted as a p-value.
